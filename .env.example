# ============================================
# VectorForge Environment Configuration
# Author: projectfong
# ============================================

# [!] Warning:
# These credentials are for local development and testing only.
# Replace with secure values for any production or public deployment.

# -------------------------------
# PostgreSQL (pgvector)
# -------------------------------
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_DB=spacebio
POSTGRES_USER=spaceapps
POSTGRES_PASSWORD=spaceapps
DATABASE_URL=postgresql://spaceapps:spaceapps@postgres:5432/spacebio

# -------------------------------
# Qdrant Vector Database
# -------------------------------
QDRANT_HOST=qdrant
QDRANT_PORT=6333
QDRANT_URL=http://qdrant:6333

# -------------------------------
# External Inference (Ollama)
# -------------------------------
# Make sure Ollama is running locally or on a reachable host.
# Example: `ollama serve` or specify LAN IP if remote.
OLLAMA_HOST=http://ollama.local:11434
OLLAMA_EMBED_MODEL=snowflake-arctic-embed2:568m
OLLAMA_CHAT_MODEL=llama3.1:8b-instruct-q4_0

# -------------------------------
# Optional OpenAI (fallback)
# -------------------------------
# If an API key is present, backend will auto-enable OpenAI routes.
OPENAI_API_KEY=
OPENAI_CHAT_MODEL=gpt-4o-mini
OPENAI_EMBED_MODEL=text-embedding-3-large

# -------------------------------
# Frontend â†’ Backend Connection
# -------------------------------
# Used by Vite during build. Ensure this matches your backend port.
VITE_API_URL=http://localhost:8000

# -------------------------------
# Search Configuration
# -------------------------------
DEFAULT_TOPK=10
